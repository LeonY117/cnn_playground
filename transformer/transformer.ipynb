{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import transformers\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Any, Callable, List, Optional\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset\n",
    "\n",
    "simple wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains 205328 titles\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "253c1d62178c494a8bbecf207967c91f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "simple_wiki_dataset = load_dataset('wikipedia', \"20220301.simple\") # only contains training set since it's lanaugage modeling\n",
    "\n",
    "print(f\"Dataset contains {len(simple_wiki_dataset['train']['title'])} titles\")\n",
    "\n",
    "\n",
    "# take text from first 1000 titles \n",
    "training_text = simple_wiki_dataset['train']['text'][:1000]\n",
    "\n",
    "# get unique characters from text\n",
    "unique_chars = []\n",
    "for text in tqdm(training_text):\n",
    "    for char in text:\n",
    "        if char not in unique_chars:\n",
    "            unique_chars.append(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "472"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A thousand (1000, one thousand or 1,000) is the natural number after 999 and before 1001. One thousand thousands is known as a million.\\n\\nIn Roman numerals, 1000 is written as M.\\n\\nExamples of a thousand \\n The number of grams in a kilogram\\n The number of millimeters in a meter\\n The number of years in a millennium\\n\\n19E03 1000'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [(i, len(x)) for i, x in enumerate(text)]\n",
    "text[917]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess dataset\n",
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# character level encoding and decoding\n",
    "class CharacterTokenizer:\n",
    "    def __init__(self, vocab: List[str]):\n",
    "        self.vocab = vocab\n",
    "        self.char2idx = {char: idx for idx, char in enumerate(self.vocab)}\n",
    "        self.idx2char = {idx: char for idx, char in enumerate(self.vocab)}\n",
    "    \n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        return [self.char2idx[char] for char in text]\n",
    "    \n",
    "    def decode(self, tokens: List[int]) -> str:\n",
    "        return ''.join([self.idx2char[idx] for idx in tokens])\n",
    "\n",
    "# create a character tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 205328/205328 [01:04<00:00, 3203.58 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# # BERT tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# # tokenize dataset\n",
    "# tokenized_dataset = simple_wiki_dataset.map(\n",
    "#     lambda x: tokenizer(x[\"text\"], truncation=True, padding=\"max_length\", max_length=512),\n",
    "#     batched=True,\n",
    "#     remove_columns=[\"text\"],\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim=512, embed_dim=64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_k = embed_dim\n",
    "\n",
    "        self.Wq = nn.Linear(input_dim, self.d_k, bias=False)\n",
    "        self.Wk = nn.Linear(input_dim, self.d_k, bias=False)\n",
    "        self.Wv = nn.Linear(input_dim, self.d_k, bias=False)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (B, T, C)\n",
    "        Returns:\n",
    "            attention: Tensor of shape (B, T, C)\n",
    "        \"\"\"\n",
    "        q = self.Wq(x)  # d_k x n\n",
    "        k = self.Wk(x)\n",
    "        v = self.Wv(x)\n",
    "\n",
    "        # scaled dot product attention\n",
    "\n",
    "        attention = self.softmax(q @ k.T / self.d_k**0.5) * v\n",
    "\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multihead Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
