{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "# from torchvision.ops import Conv2dNormActivation\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Any, Callable, List, Optional, Sequence, Tuple, Union\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model, showTable=False):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params += params\n",
    "    if showTable:\n",
    "        print(table)\n",
    "    return total_params\n",
    "\n",
    "def calculate_storage(model):\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    print(f'Buffer size: {buffer_size/1024**2:.3f} MB')\n",
    "\n",
    "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "    return size_all_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Conv2dNormActivation(nn.Sequential):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: Union[int, Tuple[int, ...]] = 3,\n",
    "        stride: Union[int, Tuple[int, ...]] = 1,\n",
    "        padding: Optional[Union[int, Tuple[int, ...], str]] = None,\n",
    "        groups: int = 1,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "        activation_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "        dilation: Union[int, Tuple[int, ...]] = 1,\n",
    "        bias: Optional[bool] = None,\n",
    "        inplace: Optional[bool] = True,\n",
    "    ) -> None:\n",
    "        if padding is None:\n",
    "            if dilation > 1:\n",
    "                padding = dilation * (kernel_size - 1) // 2\n",
    "            else:\n",
    "                padding = kernel_size // 2\n",
    "\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if activation_layer is None:\n",
    "            activation_layer = nn.ReLU6\n",
    "\n",
    "        layers = [\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                dilation,\n",
    "                groups,\n",
    "                bias,\n",
    "            ),\n",
    "            norm_layer(out_channels),\n",
    "            activation_layer(inplace=inplace),\n",
    "        ]\n",
    "\n",
    "        super().__init__(*layers)\n",
    "\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp: int, oup: int, stride: int, expand_ratio: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_dim = int(round(inp * expand_ratio))\n",
    "        self.use_residual = inp == oup\n",
    "\n",
    "        layers: List[nn.Module] = []\n",
    "\n",
    "        if expand_ratio != 1:\n",
    "            # pointwise expansion\n",
    "            layers.append(\n",
    "                Conv2dNormActivation(inp, hidden_dim, kernel_size=1, bias=False)\n",
    "            )\n",
    "\n",
    "        def conv_dw(inp, oup, stride):\n",
    "            # inp, oup = int(inp * self.alpha), int(oup * self.alpha)\n",
    "            return nn.Sequential(\n",
    "                # depth wise\n",
    "                Conv2dNormActivation(inp, inp, 3, stride, padding=1, groups=inp, bias=False),\n",
    "                # pointwise\n",
    "                nn.Conv2d(inp, oup, 1, 1, padding=0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "\n",
    "        # depth-wise convolution:\n",
    "        layers.append(conv_dw(hidden_dim, oup, stride))\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        if self.use_residual:\n",
    "            out = x + self.conv(x)\n",
    "        else:\n",
    "            out = self.conv(x)\n",
    "        print(out.shape)\n",
    "        return out\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install FLOWERS102 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = torchvision.transforms.Compose([\n",
    "    # resize to 256x256:\n",
    "    torchvision.transforms.Resize(256),\n",
    "    # center crop to 224x224:\n",
    "    torchvision.transforms.CenterCrop(224),\n",
    "    # convert to tensor:\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "flowers102 = {}\n",
    "flowers102['train'] = torchvision.datasets.Flowers102(root='../data', split='train', transform=transforms, download=True)\n",
    "flowers102['val'] = torchvision.datasets.Flowers102(root='../data', split='train', transform=transforms, download=True)\n",
    "flowers102['test'] = torchvision.datasets.Flowers102(root='../data', split='test', transform=transforms, download=True)\n",
    "\n",
    "dataloaders = {}\n",
    "for phase in ['train', 'val', 'test']:\n",
    "    dataloaders[phase] = torch.utils.data.DataLoader(flowers102[phase], batch_size=8, shuffle=True, num_workers=4)\n",
    "\n",
    "# get label names by reading them from '../data/flowers-102/oxford_flower_102_name.csv':\n",
    "with open('../data/flowers-102/oxford_flower_102_name.csv') as f:\n",
    "    labels = np.array([line.strip().split(',')[1] for line in f.readlines()[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 224, 224]) torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "x_batch, y_batch = next(iter(dataloaders['train']))\n",
    "print(x_batch.shape, y_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show grid of images using torchvision.utils.make_grid\n",
    "plt.figure(figsize=(16, 12))\n",
    "plt.imshow(np.transpose(torchvision.utils.make_grid(x_batch, nrow=4, padding=8, pad_value=1).numpy(), (1, 2, 0)))\n",
    "plt.axis('off')\n",
    "\n",
    "print(labels[[y.item() for y in y_batch]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard CNN\n",
    "\n",
    "input: `224 x 224 x 3`\n",
    "\n",
    "output: `102 x 1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 3 -> 32 (3x3, s=2, p=1), params = 864\n",
      "1: 32 -> 64 (3x3, s=1, p=1), params = 18432\n",
      "2: 64 -> 128 (3x3, s=2, p=1), params = 73728\n",
      "3: 128 -> 128 (3x3, s=1, p=1), params = 147456\n",
      "4: 128 -> 256 (3x3, s=2, p=1), params = 294912\n",
      "5: 256 -> 256 (3x3, s=1, p=1), params = 589824\n",
      "6: 256 -> 512 (3x3, s=2, p=1), params = 1179648\n",
      "7: 512 -> 512 (3x3, s=1, p=1), params = 2359296\n",
      "8: 512 -> 512 (3x3, s=1, p=1), params = 2359296\n",
      "9: 512 -> 512 (3x3, s=1, p=1), params = 2359296\n",
      "10: 512 -> 512 (3x3, s=1, p=1), params = 2359296\n",
      "11: 512 -> 512 (3x3, s=1, p=1), params = 2359296\n",
      "12: 512 -> 1024 (3x3, s=2, p=1), params = 4718592\n",
      "13: 1024 -> 1024 (3x3, s=1, p=1), params = 9437184\n",
      "Total number of parameters: 28379622\n"
     ]
    }
   ],
   "source": [
    "CONFIG = [\n",
    "    [3, 32, 3, 2, 1],  \n",
    "    [32, 64, 3, 1, 1],\n",
    "    [64, 128, 3, 2, 1], \n",
    "    [128, 128, 3, 1, 1], \n",
    "    [128, 256, 3, 2, 1], \n",
    "    [256, 256, 3, 1, 1], \n",
    "    [256, 512, 3, 2, 1], \n",
    "    [512, 512, 3, 1, 1], \n",
    "    [512, 512, 3, 1, 1], \n",
    "    [512, 512, 3, 1, 1], \n",
    "    [512, 512, 3, 1, 1], \n",
    "    [512, 512, 3, 1, 1], \n",
    "    [512, 1024, 3, 2, 1],\n",
    "    [1024, 1024, 3, 1, 1],\n",
    "]\n",
    "\n",
    "# manually calculate number of parameters for a standard cnn network:\n",
    "total_params = 0\n",
    "for i, (n_in, n_out, k, s, p) in enumerate(CONFIG):\n",
    "    print(f'{i}: {n_in} -> {n_out} ({k}x{k}, s={s}, p={p}), params = {n_in*n_out*k*k}')\n",
    "    total_params += n_in*n_out*k*k + n_out # +n_out for bias\n",
    "    total_params += 2*n_out # batchnorm2d learnable parameters:\n",
    "\n",
    "total_params += 1024*102 + 102 # fc layer\n",
    "print(f'Total number of parameters: {total_params}') # 27641190"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StandardCNN, self).__init__()\n",
    "        # input_channels, output_channels, kernel_size, stride, padding\n",
    "        \n",
    "        def conv(n_in, n_out, s):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(n_in, n_out, 3, s, 1),\n",
    "                nn.BatchNorm2d(n_out),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            conv(3, 32, 2), # 224x224x3 -> 112x112x32,\n",
    "            conv(32, 64, 1), # 112x112x32 -> 112x112x64\n",
    "            conv(64, 128, 2), # 112x112x64 -> 56x56x128\n",
    "            conv(128, 128, 1), # 56x56x128 -> 56x56x128\n",
    "            conv(128, 256, 2), # 56x56x128 -> 28x28x256\n",
    "            conv(256, 256, 1), # 28x28x256 -> 28x28x256\n",
    "            conv(256, 512, 2), # 28x28x256 -> 14x14x512\n",
    "            conv(512, 512, 1), # 14x14x512 -> 14x14x512\n",
    "            conv(512, 512, 1), # 14x14x512 -> 14x14x512\n",
    "            conv(512, 512, 1), # 14x14x512 -> 14x14x512\n",
    "            conv(512, 512, 1), # 14x14x512 -> 14x14x512\n",
    "            conv(512, 512, 1), # 14x14x512 -> 14x14x512\n",
    "            conv(512, 1024, 2), # 14x14x512 -> 7x7x1024\n",
    "            conv(1024, 1024, 1), # 7x7x1024 -> 7x7x1024\n",
    "        )\n",
    "        self.avg_pool = nn.AvgPool2d(7)\n",
    "        self.predictor = nn.Linear(1024, 102)\n",
    "\n",
    "    def _build_conv_layers(self, config):\n",
    "        # build conv layers from nested dictionary:\n",
    "        layers = []\n",
    "        for i, (input_channels, output_channels, kernel_size, stride, padding) in enumerate(config):\n",
    "            block = nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride, padding),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "            layers.append(block)\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x): \n",
    "        x = self.conv(x)               # 224x224x3 -> 7x7x1024\n",
    "        print(x.shape)\n",
    "        x = self.avg_pool(x).squeeze() # 7x7x1024 -> 1024\n",
    "        x = self.predictor(x)          # 1024 -> 102\n",
    "        return x\n",
    "\n",
    "standard_cnn = StandardCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1024, 7, 7])\n",
      "output dimension: torch.Size([8, 102])\n",
      "Buffer size: 0.046 MB\n",
      "model size: 108.305MB\n",
      "Total Trainable Params: 28379622\n"
     ]
    }
   ],
   "source": [
    "# check if the model produces the expected output shape:\n",
    "x_batch = torch.zeros(8, 3, 224, 224)\n",
    "print(f'output dimension: {standard_cnn(x_batch).shape}')\n",
    "\n",
    "size_all_mb = calculate_storage(standard_cnn)\n",
    "print('model size: {:.3f}MB'.format(size_all_mb)) #105.486MB\n",
    "\n",
    "total_params = count_parameters(standard_cnn)\n",
    "print(f\"Total Trainable Params: {total_params}\") #27641190"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MobileNetV1\n",
    "\n",
    "Depth-wise separable filters: \n",
    "The idea is to decouple the filtering and the combination steps into two separate steps, which are normally done in one go for a usual conv layer\n",
    "\n",
    "The `alpha` parameter controls the depth of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileNetV1(nn.Module):\n",
    "    def __init__(self, alpha):\n",
    "        super(MobileNetV1, self).__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "        def conv(inp, oup, stride):\n",
    "            inp, oup = inp, int(oup*self.alpha)\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(inp, oup, kernel_size=3, stride=stride, padding=1),\n",
    "                nn.BatchNorm2d(oup),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "\n",
    "        def conv_dw(inp, oup, stride):\n",
    "            inp, oup = int(inp*self.alpha), int(oup*self.alpha)\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(inp, inp, kernel_size=3, stride=stride, padding=1, groups=inp, bias=False),\n",
    "                nn.BatchNorm2d(inp),\n",
    "                nn.ReLU(inplace=True),\n",
    "\n",
    "                nn.Conv2d(inp, oup, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "\n",
    "        # input_channels, output_channels, stride\n",
    "        self.conv = nn.Sequential(\n",
    "            conv(3, 32, 2), # 224x224x3 -> 112x112x32,\n",
    "            conv_dw(32, 64, 1), # 112x112x32 -> 112x112x64\n",
    "            conv_dw(64, 128, 2), # 112x112x64 -> 56x56x128\n",
    "            conv_dw(128, 128, 1), # 56x56x128 -> 56x56x128\n",
    "            conv_dw(128, 256, 2), # 56x56x128 -> 28x28x256\n",
    "            conv_dw(128, 256, 1), # 28x28x256 -> 28x28x256\n",
    "            conv_dw(256, 512, 2), # 28x28x256 -> 14x14x512\n",
    "            conv_dw(512, 512, 1), # 14x14x512 -> 14x14x512\n",
    "            conv_dw(512, 512, 1), # 14x14x512 -> 14x14x512\n",
    "            conv_dw(512, 512, 1), # 14x14x512 -> 14x14x512\n",
    "            conv_dw(512, 512, 1), # 14x14x512 -> 14x14x512\n",
    "            conv_dw(512, 512, 1), # 14x14x512 -> 14x14x512\n",
    "            conv_dw(512, 1024, 2), # 14x14x512 -> 7x7x1024\n",
    "            conv_dw(1024, 1024, 1), # 7x7x1024 -> 7x7x1024\n",
    "        )\n",
    "        self.avg_pool = nn.AvgPool2d(7)\n",
    "        self.predictor = nn.Linear(1024, 102)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)  # 224x224x3 -> 7x7x1024\n",
    "        x = self.avg_pool(x).squeeze()  # 7x7x1024 -> 1024\n",
    "        x = self.predictor(x)  # 1024 -> 102\n",
    "        return x\n",
    "\n",
    "\n",
    "mobilenet_v1 = MobileNetV1(alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buffer size: 0.046 MB\n",
      "model size: 108.305MB\n",
      "Buffer size: 0.083 MB\n",
      "model size: 12.585MB\n",
      "Total Trainable Params: 28379622\n",
      "Total Trainable Params: 3277382\n"
     ]
    }
   ],
   "source": [
    "size_all_mb = calculate_storage(standard_cnn)\n",
    "print('model size: {:.3f}MB'.format(size_all_mb)) # 105.486MB\n",
    "\n",
    "size_all_mb = calculate_storage(mobilenet_v1)\n",
    "print('model size: {:.3f}MB'.format(size_all_mb)) # 12.585MB\n",
    "\n",
    "total_params = count_parameters(standard_cnn, False)\n",
    "print(f\"Total Trainable Params: {total_params}\") # 27641190\n",
    "\n",
    "total_params = count_parameters(mobilenet_v1, False)\n",
    "print(f\"Total Trainable Params: {total_params}\") # 3277382"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mobilenet_v1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MobileNet V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from mobilenet import Conv2dNormActivation, InvertedResidual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, alpha: float) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        config = [\n",
    "            # t, c, n, s\n",
    "            [1, 16, 1, 1],\n",
    "            [6, 24, 2, 2],\n",
    "            [6, 32, 3, 2],\n",
    "            [6, 64, 4, 2],\n",
    "            [6, 96, 3, 1],\n",
    "            [6, 160, 3, 2],\n",
    "            [6, 320, 1, 1],\n",
    "        ]\n",
    "\n",
    "        # create encoder and add first layer\n",
    "        encoder: List[nn.Module] = [Conv2dNormActivation(3, 32, 3, 2, 1, bias=False)]\n",
    "\n",
    "        # add bottleneck layers\n",
    "        input_channels = 32\n",
    "        for t, c, n, s in config:\n",
    "            for i in range(n):\n",
    "                s = s if i == 0 else 1\n",
    "                encoder.append(InvertedResidual(input_channels, c, s, t))\n",
    "                input_channels = c\n",
    "\n",
    "        # append last conv2d\n",
    "        encoder.append(Conv2dNormActivation(input_channels, 1280, 1, 1, bias=False))\n",
    "\n",
    "        self.encoder = nn.Sequential(*encoder)\n",
    "\n",
    "        # build classifier\n",
    "        # self.avg_pool = nn.AvgPool2d(7)\n",
    "        self.predictor = nn.Linear(1280, 102)\n",
    "\n",
    "        # weight initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\")\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.encoder(x)\n",
    "        # x = self.avg_pool(x)\n",
    "        # x = torch.flatten(x, 1)\n",
    "        # x = self.predictor(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "mobilenet_v2 = MobileNetV2(alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 112, 112])\n",
      "torch.Size([1, 24, 56, 56])\n",
      "torch.Size([1, 24, 56, 56])\n",
      "torch.Size([1, 32, 28, 28])\n",
      "torch.Size([1, 32, 28, 28])\n",
      "torch.Size([1, 32, 28, 28])\n",
      "torch.Size([1, 64, 14, 14])\n",
      "torch.Size([1, 64, 14, 14])\n",
      "torch.Size([1, 64, 14, 14])\n",
      "torch.Size([1, 64, 14, 14])\n",
      "torch.Size([1, 96, 14, 14])\n",
      "torch.Size([1, 96, 14, 14])\n",
      "torch.Size([1, 96, 14, 14])\n",
      "torch.Size([1, 160, 7, 7])\n",
      "torch.Size([1, 160, 7, 7])\n",
      "torch.Size([1, 160, 7, 7])\n",
      "torch.Size([1, 320, 7, 7])\n",
      "output dimension: torch.Size([1, 1280, 7, 7])\n",
      "Buffer size: 0.131 MB\n",
      "model size: 9.112 MB\n",
      "Total Trainable Params: 2354534\n"
     ]
    }
   ],
   "source": [
    "# check if the model produces the expected output shape:\n",
    "x_batch = torch.zeros(1, 3, 224, 224)\n",
    "print(f'output dimension: {mobilenet_v2(x_batch).shape}')\n",
    "\n",
    "size_all_mb = calculate_storage(mobilenet_v2)\n",
    "print('model size: {:.3f} MB'.format(size_all_mb)) # 8.614 MB\n",
    "\n",
    "total_params = count_parameters(mobilenet_v2, False)\n",
    "print(f\"Total Trainable Params: {total_params}\") # 2223872"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buffer size: 0.131 MB\n",
      "model size: 13.501 MB\n",
      "Total Trainable Params: 3504872\n"
     ]
    }
   ],
   "source": [
    "# load mobilenetV2 pertrained from torchvision:\n",
    "\n",
    "mobilenet_v2_pretrained = torchvision.models.mobilenet_v2()\n",
    "\n",
    "size_all_mb = calculate_storage(mobilenet_v2_pretrained)\n",
    "print('model size: {:.3f} MB'.format(size_all_mb)) # 13.501 MB\n",
    "\n",
    "total_params = count_parameters(mobilenet_v2_pretrained, False)\n",
    "print(f\"Total Trainable Params: {total_params}\") # 3504872"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MobileNetV2(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (12): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (13): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (14): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (15): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (16): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (17): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (18): Conv2dNormActivation(\n",
       "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=False)\n",
       "    (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mobilenet_v2_pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
