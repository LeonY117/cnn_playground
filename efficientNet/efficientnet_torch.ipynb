{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "import copy\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "from torchvision.ops import StochasticDepth\n",
    "from torchvision.ops.misc import Conv2dNormActivation, SqueezeExcitation\n",
    "\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Any, Callable, List, Optional, Sequence, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "b0_config = [\n",
    "    # t, k, c, n, s\n",
    "    [1, 3, 16, 1, 1],  # 112 x 112\n",
    "    [6, 3, 24, 2, 2],  # 112 x 112\n",
    "    [6, 5, 40, 2, 2],  # 56 x 56\n",
    "    [6, 3, 80, 3, 2],  # 28 x 28\n",
    "    [6, 5, 112, 3, 1],  # 28 x 28\n",
    "    [6, 5, 192, 4, 2],  # 14 x 14\n",
    "    [6, 3, 320, 1, 1],  # 7 x 7\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_divisible(v: float, divisor: int, min_value: Optional[int] = None) -> int:\n",
    "    \"\"\"\n",
    "    This function is taken from the original tf repo.\n",
    "    It ensures that all layers have a channel number that is divisible by 8\n",
    "    It can be seen here:\n",
    "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class _MBConvConfig:\n",
    "    expand_ratio: float\n",
    "    kernel: int\n",
    "    stride: int\n",
    "    input_channels: int\n",
    "    out_channels: int\n",
    "    num_layers: int\n",
    "    block: Callable[..., nn.Module]\n",
    "\n",
    "    @staticmethod\n",
    "    def adjust_channels(\n",
    "        channels: int, width_mult: float, min_value: Optional[int] = None\n",
    "    ) -> int:\n",
    "        return _make_divisible(channels * width_mult, 8, min_value)\n",
    "\n",
    "    @staticmethod\n",
    "    def adjust_depth(num_layers: int, depth_mult: float):\n",
    "        return int(math.ceil(num_layers * depth_mult))\n",
    "\n",
    "\n",
    "class MBConvConfig(_MBConvConfig):\n",
    "    # Stores information listed at Table 1 of the EfficientNet paper\n",
    "    def __init__(\n",
    "        self,\n",
    "        expand_ratio: float,\n",
    "        kernel: int,\n",
    "        stride: int,\n",
    "        input_channels: int,\n",
    "        out_channels: int,\n",
    "        num_layers: int,\n",
    "        width_mult: float = 1.0,\n",
    "        depth_mult: float = 1.0,\n",
    "        block: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        input_channels = self.adjust_channels(input_channels, width_mult)\n",
    "        out_channels = self.adjust_channels(out_channels, width_mult)\n",
    "        num_layers = self.adjust_depth(num_layers, depth_mult)\n",
    "        if block is None:\n",
    "            block = MBConv\n",
    "        super().__init__(\n",
    "            expand_ratio,\n",
    "            kernel,\n",
    "            stride,\n",
    "            input_channels,\n",
    "            out_channels,\n",
    "            num_layers,\n",
    "            block,\n",
    "        )\n",
    "\n",
    "\n",
    "class MBConv(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg,\n",
    "        stochastic_depth_prob: float,\n",
    "        norm_layer: Callable[..., nn.Module],\n",
    "        se_layer: Callable[..., nn.Module],\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        if not (1 <= cfg.stride <= 2):\n",
    "            raise ValueError(\"illegal stride value\")\n",
    "\n",
    "        self.use_res_connect = (\n",
    "            cfg.stride == 1 and cfg.input_channels == cfg.out_channels\n",
    "        )\n",
    "\n",
    "        layers: List[nn.Module] = []\n",
    "        activation_layer = nn.SiLU\n",
    "\n",
    "        # expand\n",
    "        expanded_channels = cfg.adjust_channels(cfg.input_channels, cfg.expand_ratio)\n",
    "        if expanded_channels != cfg.input_channels:\n",
    "            layers.append(\n",
    "                Conv2dNormActivation(\n",
    "                    cfg.input_channels,\n",
    "                    expanded_channels,\n",
    "                    kernel_size=1,\n",
    "                    norm_layer=norm_layer,\n",
    "                    activation_layer=activation_layer,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # depthwise\n",
    "        layers.append(\n",
    "            Conv2dNormActivation(\n",
    "                expanded_channels,\n",
    "                expanded_channels,\n",
    "                kernel_size=cfg.kernel,\n",
    "                stride=cfg.stride,\n",
    "                groups=expanded_channels,\n",
    "                norm_layer=norm_layer,\n",
    "                activation_layer=activation_layer,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # squeeze and excitation\n",
    "        squeeze_channels = max(1, cfg.input_channels // 4)\n",
    "        layers.append(\n",
    "            se_layer(\n",
    "                expanded_channels,\n",
    "                squeeze_channels,\n",
    "                activation=partial(nn.SiLU, inplace=True),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # project\n",
    "        layers.append(\n",
    "            Conv2dNormActivation(\n",
    "                expanded_channels,\n",
    "                cfg.out_channels,\n",
    "                kernel_size=1,\n",
    "                norm_layer=norm_layer,\n",
    "                activation_layer=None,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.block = nn.Sequential(*layers)\n",
    "        self.stochastic_depth = StochasticDepth(stochastic_depth_prob, \"row\")\n",
    "        self.out_channels = cfg.out_channels\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        result = self.block(input)\n",
    "        if self.use_res_connect:\n",
    "            result = self.stochastic_depth(result)\n",
    "            result += input\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        inverted_residual_setting: Sequence[MBConvConfig],\n",
    "        dropout: float,\n",
    "        stochastic_depth_prob: float = 0.2,\n",
    "        num_classes: int = 1000,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = nn.BatchNorm2d,\n",
    "        last_channel: Optional[int] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inverted_residual_setting (Sequence[Union[MBConvConfig, FusedMBConvConfig]]): Network structure\n",
    "            dropout (float): The dropout probability\n",
    "            stochastic_depth_prob (float): The stochastic depth probability\n",
    "            num_classes (int): Number of classes\n",
    "            norm_layer (Optional[Callable[..., nn.Module]]): Module specifying the normalization layer to use\n",
    "            last_channel (int): The number of channels on the penultimate layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        layers: List[nn.Module] = []\n",
    "\n",
    "        # Build first layer\n",
    "        firstconv_output_channels = inverted_residual_setting[0].input_channels\n",
    "        layers.append(\n",
    "            Conv2dNormActivation(\n",
    "                3,\n",
    "                firstconv_output_channels,\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "                norm_layer=norm_layer,\n",
    "                activation_layer=nn.SiLU,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Build inverted residual blocks\n",
    "        total_stage_blocks = sum(cfg.num_layers for cfg in inverted_residual_setting)\n",
    "        stage_block_id = 0\n",
    "        for cfg in inverted_residual_setting:\n",
    "            stage: List[nn.Module] = []\n",
    "            for _ in range(cfg.num_layers):\n",
    "                # copy to avoid modifications. shallow copy is enough\n",
    "                block_cfg = copy.copy(cfg)\n",
    "\n",
    "                # overwrite info if not the first conv in the stage\n",
    "                if stage:\n",
    "                    block_cfg.input_channels = block_cfg.out_channels\n",
    "                    block_cfg.stride = 1\n",
    "\n",
    "                # adjust stochastic depth probability based on the depth of the stage block\n",
    "                sd_prob = (\n",
    "                    stochastic_depth_prob * float(stage_block_id) / total_stage_blocks\n",
    "                )\n",
    "\n",
    "                stage.append(block_cfg.block(block_cfg, sd_prob, norm_layer))\n",
    "                stage_block_id += 1\n",
    "\n",
    "            layers.append(nn.Sequential(*stage))\n",
    "\n",
    "        # Build last several layers\n",
    "        lastconv_input_channels = inverted_residual_setting[-1].out_channels\n",
    "        lastconv_output_channels = (\n",
    "            last_channel if last_channel is not None else 4 * lastconv_input_channels\n",
    "        )\n",
    "        layers.append(\n",
    "            Conv2dNormActivation(\n",
    "                lastconv_input_channels,\n",
    "                lastconv_output_channels,\n",
    "                kernel_size=1,\n",
    "                norm_layer=norm_layer,\n",
    "                activation_layer=nn.SiLU,\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SNet_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
