{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient Unet\n",
    "\n",
    "My attempt at building a more efficient unet using depthwise separable deconv blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from prettytable import PrettyTable\n",
    "from collections import OrderedDict\n",
    "from typing import Any, Callable, List, Optional, Sequence, Tuple, Union\n",
    "\n",
    "from misc import Conv2dNormActivation, InvertedResidual\n",
    "\n",
    "\n",
    "def count_parameters(model, showTable=False):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params += params\n",
    "    if showTable:\n",
    "        print(table)\n",
    "    return total_params\n",
    "\n",
    "\n",
    "def calculate_storage(model, show_buffer=True):\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    if show_buffer:\n",
    "        print(f\"Buffer size: {buffer_size/1024**2:.3f} MB\")\n",
    "\n",
    "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "    return size_all_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "\n",
    "from typing import Any, Callable, List, Optional, Tuple, Union\n",
    "\n",
    "\n",
    "class Conv2dNormActivation(nn.Sequential):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: Union[int, Tuple[int, ...]] = 3,\n",
    "        stride: Union[int, Tuple[int, ...]] = 1,\n",
    "        padding: Optional[Union[int, Tuple[int, ...], str]] = None,\n",
    "        groups: int = 1,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "        activation_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "        dropout_p: float = 0.0,\n",
    "        dilation: Union[int, Tuple[int, ...]] = 1,\n",
    "        bias: Optional[bool] = None,\n",
    "        inplace: Optional[bool] = True,\n",
    "    ) -> None:\n",
    "        if padding is None:\n",
    "            if dilation > 1:\n",
    "                padding = dilation * (kernel_size - 1) // 2\n",
    "            else:\n",
    "                padding = kernel_size // 2\n",
    "\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if activation_layer is None:\n",
    "            activation_layer = nn.ReLU6\n",
    "\n",
    "        dropout_layer = nn.Dropout2d\n",
    "\n",
    "        layers = [\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                dilation,\n",
    "                groups,\n",
    "                bias,\n",
    "            ),\n",
    "            norm_layer(out_channels),\n",
    "            dropout_layer(p=dropout_p),\n",
    "            activation_layer(inplace=inplace),\n",
    "        ]\n",
    "\n",
    "        super().__init__(*layers)\n",
    "\n",
    "\n",
    "class Deconv2dNormActivation(nn.Sequential):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: Union[int, Tuple[int, ...]] = 3,\n",
    "        stride: Union[int, Tuple[int, ...]] = 1,\n",
    "        padding: Optional[Union[int, Tuple[int, ...], str]] = None,\n",
    "        output_padding: Optional[Union[int, Tuple[int, ...], str]] = None,\n",
    "        groups: int = 1,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "        activation_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "        dropout_p: float = 0.0,\n",
    "        dilation: Union[int, Tuple[int, ...]] = 1,\n",
    "        bias: Optional[bool] = None,\n",
    "        inplace: Optional[bool] = True,\n",
    "    ) -> None:\n",
    "        if padding is None:\n",
    "            if dilation > 1:\n",
    "                padding = dilation * (kernel_size - 1) // 2\n",
    "            else:\n",
    "                padding = kernel_size // 2\n",
    "\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if activation_layer is None:\n",
    "            activation_layer = nn.ReLU6\n",
    "\n",
    "        dropout_layer = nn.Dropout2d\n",
    "\n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                output_padding,\n",
    "                groups,\n",
    "                bias,\n",
    "                dilation,\n",
    "            ),\n",
    "            norm_layer(out_channels),\n",
    "            dropout_layer(p=dropout_p),\n",
    "            activation_layer(inplace=inplace),\n",
    "        ]\n",
    "\n",
    "        super().__init__(*layers)\n",
    "\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp: int, oup: int, stride: int, expand_ratio: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_dim = int(round(inp * expand_ratio))\n",
    "        self.use_residual = inp == oup\n",
    "\n",
    "        layers: List[nn.Module] = []\n",
    "\n",
    "        if expand_ratio != 1:\n",
    "            # pointwise expansion\n",
    "            layers.append(\n",
    "                Conv2dNormActivation(inp, hidden_dim, kernel_size=1, bias=False)\n",
    "            )\n",
    "\n",
    "        def conv_dw(inp, oup, stride):\n",
    "            # inp, oup = int(inp * self.alpha), int(oup * self.alpha)\n",
    "            return nn.Sequential(\n",
    "                # depth wise\n",
    "                Conv2dNormActivation(\n",
    "                    inp,\n",
    "                    inp,\n",
    "                    kernel_size=3,\n",
    "                    stride=stride,\n",
    "                    padding=1,\n",
    "                    groups=inp,\n",
    "                    bias=False,\n",
    "                    norm_layer=nn.BatchNorm2d,\n",
    "                    activation_layer=nn.ReLU6,\n",
    "                ),\n",
    "                # pointwise\n",
    "                nn.Conv2d(inp, oup, 1, 1, padding=0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "\n",
    "        # depth-wise convolution:\n",
    "        layers.append(conv_dw(hidden_dim, oup, stride))\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        out = self.conv(x)\n",
    "        if self.use_residual:\n",
    "            out = x + out\n",
    "        print(out.shape)\n",
    "        return out\n",
    "\n",
    "\n",
    "def deconv_dw(inp: int, oup: int, stride: int) -> nn.Sequential:\n",
    "    # inp, oup = int(inp * self.alpha), int(oup * self.alpha)\n",
    "    out_pad = 1 if stride == 2 else 0\n",
    "    return nn.Sequential(\n",
    "        # depth wise\n",
    "        Deconv2dNormActivation(\n",
    "            inp,\n",
    "            inp,\n",
    "            3,\n",
    "            stride=stride,\n",
    "            padding=1,\n",
    "            output_padding=out_pad,\n",
    "            groups=inp,\n",
    "            bias=False,\n",
    "            norm_layer=nn.BatchNorm2d,\n",
    "            activation_layer=nn.ReLU6,\n",
    "        ),\n",
    "        # pointwise\n",
    "        nn.Conv2d(inp, oup, 1, 1, padding=0, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "    )\n",
    "\n",
    "\n",
    "class DecomposedDeconv(nn.Module):\n",
    "    # allow arbitrary input and output channel numbers\n",
    "    def __init__(self, inp: int, oup: int, stride: int, *args) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.deconv = nn.Sequential(deconv_dw(inp, oup, stride))\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        out = self.deconv(x)\n",
    "        print(out.shape)\n",
    "        return out\n",
    "\n",
    "\n",
    "class InvertedResidualDeconv(nn.Module):\n",
    "    def __init__(self, inp: int, oup: int, stride: int, expand_ratio: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_dim = int(round(inp * expand_ratio))\n",
    "        self.use_residual = inp == oup\n",
    "\n",
    "        layers: List[nn.Module] = []\n",
    "\n",
    "        if expand_ratio != 1:\n",
    "            # pointwise expansion\n",
    "            layers.append(\n",
    "                Conv2dNormActivation(inp, hidden_dim, kernel_size=1, bias=False)\n",
    "            )\n",
    "\n",
    "        layers.append(deconv_dw(hidden_dim, oup, stride))\n",
    "        self.deconv = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        out = self.deconv(x)\n",
    "        if self.use_residual:\n",
    "            out = x + out\n",
    "        print(out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cfg = [\n",
    "    [0, 0, 0, 0, 0, 0, 0.5, 0.5, 0.5],\n",
    "    [0.5, 0.5, 0.5, 0, 0, 0, 0, 0, 0],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  6 160   1   1]\n",
      " [  6  96   3   2]\n",
      " [  6  64   3   1]\n",
      " [  6  32   4   2]\n",
      " [  6  24   3   2]\n",
      " [  6  16   2   2]\n",
      " [  1   3   1   1]]\n"
     ]
    }
   ],
   "source": [
    "class MobileUNet(nn.Module):\n",
    "    def __init__(self, alpha: float = 1.0) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # ================ parameters ================\n",
    "        self.hidden_dim: int = 1280\n",
    "        self.num_classes: int = 3\n",
    "        # self.config: np.ndarray = np.array(\n",
    "        #     [\n",
    "        #         # t, c, n, s\n",
    "        #         [1, 16, 1, 1],\n",
    "        #         [6, 24, 2, 2],  # skip\n",
    "        #         [6, 32, 3, 2],  # skip\n",
    "        #         [6, 64, 4, 2],  # skip\n",
    "        #         [6, 96, 3, 1],\n",
    "        #         [6, 160, 3, 2],  # skip\n",
    "        #         [6, 320, 1, 1],\n",
    "        #     ]\n",
    "        # )\n",
    "        self.config: np.ndarray = np.array(\n",
    "            [\n",
    "                # t, c, n, s, op\n",
    "                [1, 16, 1, 1, 0],\n",
    "                [6, 24, 2, 2, 1],  # skip\n",
    "                [6, 32, 3, 2, 1],  # skip\n",
    "                [6, 64, 4, 2, 0],  # skip\n",
    "                [6, 96, 3, 1, 0],\n",
    "                [6, 160, 3, 2, 0],  # skip\n",
    "                [6, 320, 1, 1, 0],\n",
    "            ]\n",
    "        )\n",
    "        encoder_block = InvertedResidual\n",
    "        decoder_block = InvertedResidualDeconv\n",
    "        dropout_block = nn.Dropout2d\n",
    "        # decoder_block = DecomposedDeconv\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.config[:, 1] = np.ceil(self.config[:, 1] * self.alpha).astype(int)\n",
    "\n",
    "        # ================== encoder ==================\n",
    "        encoder = OrderedDict()\n",
    "        inp = 32  # first conv2d channels\n",
    "        encoder[\"conv0\"] = Conv2dNormActivation(\n",
    "            3, inp, 3, 2, 1, bias=False, dropout_p=drop_cfg[0][0]\n",
    "        )\n",
    "\n",
    "        # add bottleneck layers\n",
    "        for i, (t, c, n, s) in enumerate(self.config):\n",
    "            blocks: List[nn.Module] = []\n",
    "            for j in range(n):\n",
    "                sj = s if j == 0 else 1\n",
    "                blocks.extend(\n",
    "                    [encoder_block(inp, c, sj, t), dropout_block(p=drop_cfg[0][i + 1])]\n",
    "                )\n",
    "                inp = c\n",
    "            encoder[f\"bottleneck{i+1}\"] = nn.Sequential(*blocks)\n",
    "\n",
    "        # append last conv2d\n",
    "        encoder[\"convLast\"] = Conv2dNormActivation(\n",
    "            inp, self.hidden_dim, 1, 1, bias=False, dropout_p=drop_cfg[0][-1]\n",
    "        )\n",
    "\n",
    "        # ================== decoder ==================\n",
    "        def get_deconv_config(config: np.ndarray) -> np.ndarray:\n",
    "            deconv_config = config.copy()\n",
    "            # shift channel up by 1\n",
    "            deconv_config[:, 1] = np.concatenate(([3], deconv_config[:, 1][:-1]))\n",
    "\n",
    "            return deconv_config[::-1]\n",
    "\n",
    "        self.deconv_config = get_deconv_config(self.config)\n",
    "        print(self.deconv_config)\n",
    "\n",
    "        decoder = OrderedDict()\n",
    "        decoder[\"deconv0\"] = Conv2dNormActivation(\n",
    "            self.hidden_dim, inp, 1, 1, bias=False, dropout_p=drop_cfg[1][0]\n",
    "        )\n",
    "        skip = 0\n",
    "        for i, (t, c, n, s) in enumerate(self.deconv_config):\n",
    "            blocks: List[nn.Module] = []\n",
    "            for j in range(n):\n",
    "                sj = s if j == n - 1 else 1\n",
    "                cj = c if j == n - 1 else inp\n",
    "                blocks.extend(\n",
    "                    [\n",
    "                        decoder_block(inp + skip, cj, sj, t),\n",
    "                        dropout_block(p=drop_cfg[1][i + 1]),\n",
    "                    ]\n",
    "                )\n",
    "                inp = cj\n",
    "                skip = c if sj == 2 else 0\n",
    "            decoder[f\"decomposed_deconv{i+1}\"] = nn.Sequential(*blocks)\n",
    "\n",
    "        # append last deconv2d\n",
    "        decoder[\"deconvLast\"] = Deconv2dNormActivation(\n",
    "            inp, self.num_classes, 3, 2, 1, 1, bias=False, dropout_p=drop_cfg[1][-1]\n",
    "        )\n",
    "\n",
    "        # ========= weight initialization ========\n",
    "        self.encoder = nn.Sequential(encoder)\n",
    "        self.decoder = nn.Sequential(decoder)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self) -> None:\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\")\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.ConvTranspose2d):\n",
    "                torch.nn.init.xavier_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # store features for skip connections\n",
    "        features = []\n",
    "\n",
    "        # ENCODER\n",
    "        x = self.encoder[0](x)\n",
    "        for i in range(len(self.config)):\n",
    "            print(f\"Encoder {i+1}\")\n",
    "            x = self.encoder[i + 1](x)\n",
    "            if i < len(self.config) - 1 and self.config[i + 1][3] == 2:\n",
    "                features.append(x)\n",
    "                # print(features[-1].shape)\n",
    "        # final hidden state\n",
    "        x = self.encoder[-1](x)\n",
    "        print(\"hidden state\", x.shape)\n",
    "        # DECODER\n",
    "        # first deconv\n",
    "        x = self.decoder[0](x)\n",
    "        # symmetrical decoder\n",
    "        for i in range(len(self.config)):\n",
    "            print(f\"Decoder {i+1}\")\n",
    "            x = self.decoder[i + 1](x)\n",
    "            if self.deconv_config[i][3] == 2:\n",
    "                # print(f\"{x.shape}, {features[-1].shape}\")\n",
    "                print(\"skip connection here\")\n",
    "                # skip connection\n",
    "                x = torch.cat((x, features.pop()), dim=1)\n",
    "        # final output\n",
    "        x = self.decoder[-1](x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# initialize network\n",
    "unet = MobileUNet(alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder 1\n",
      "torch.Size([1, 16, 240, 320])\n",
      "Encoder 2\n",
      "torch.Size([1, 24, 120, 160])\n",
      "torch.Size([1, 24, 120, 160])\n",
      "Encoder 3\n",
      "torch.Size([1, 32, 60, 80])\n",
      "torch.Size([1, 32, 60, 80])\n",
      "torch.Size([1, 32, 60, 80])\n",
      "Encoder 4\n",
      "torch.Size([1, 64, 30, 40])\n",
      "torch.Size([1, 64, 30, 40])\n",
      "torch.Size([1, 64, 30, 40])\n",
      "torch.Size([1, 64, 30, 40])\n",
      "Encoder 5\n",
      "torch.Size([1, 96, 30, 40])\n",
      "torch.Size([1, 96, 30, 40])\n",
      "torch.Size([1, 96, 30, 40])\n",
      "Encoder 6\n",
      "torch.Size([1, 160, 15, 20])\n",
      "torch.Size([1, 160, 15, 20])\n",
      "torch.Size([1, 160, 15, 20])\n",
      "Encoder 7\n",
      "torch.Size([1, 320, 15, 20])\n",
      "hidden state torch.Size([1, 1280, 15, 20])\n",
      "Decoder 1\n",
      "torch.Size([1, 160, 15, 20])\n",
      "Decoder 2\n",
      "torch.Size([1, 160, 15, 20])\n",
      "torch.Size([1, 160, 15, 20])\n",
      "torch.Size([1, 96, 30, 40])\n",
      "skip connection here\n",
      "Decoder 3\n",
      "torch.Size([1, 96, 30, 40])\n",
      "torch.Size([1, 96, 30, 40])\n",
      "torch.Size([1, 64, 30, 40])\n",
      "Decoder 4\n",
      "torch.Size([1, 64, 30, 40])\n",
      "torch.Size([1, 64, 30, 40])\n",
      "torch.Size([1, 64, 30, 40])\n",
      "torch.Size([1, 32, 60, 80])\n",
      "skip connection here\n",
      "Decoder 5\n",
      "torch.Size([1, 32, 60, 80])\n",
      "torch.Size([1, 32, 60, 80])\n",
      "torch.Size([1, 24, 120, 160])\n",
      "skip connection here\n",
      "Decoder 6\n",
      "torch.Size([1, 24, 120, 160])\n",
      "torch.Size([1, 16, 240, 320])\n",
      "skip connection here\n",
      "Decoder 7\n",
      "torch.Size([1, 3, 240, 320])\n",
      "output dimension: torch.Size([1, 3, 480, 640])\n"
     ]
    }
   ],
   "source": [
    "# check if the model produces the expected output shape:\n",
    "x_batch = torch.zeros(1, 3, 480, 640)\n",
    "print(f'output dimension: {unet(x_batch).shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODER:\n",
    "# bottleneck1: torch.Size([1, 16, 112, 112])\n",
    "# bottleneck2: torch.Size([1, 24, 56, 56])\n",
    "# bottleneck3: torch.Size([1, 32, 28, 28])\n",
    "# bottleneck4: torch.Size([1, 64, 14, 14])\n",
    "# bottleneck5: torch.Size([1, 96, 14, 14])\n",
    "# bottleneck6: torch.Size([1, 160, 7, 7])\n",
    "# bottleneck7: torch.Size([1, 320, 7, 7])\n",
    "# final conv:  torch.Size([1, 1280, 7, 7])\n",
    "# DECODER:\n",
    "# torch.Size([1, 320, 7, 7])\n",
    "# decoder1: torch.Size([1, 320, 7, 7])\n",
    "# decoder2: torch.Size([1, 160, 14, 14])\n",
    "# decoder3: torch.Size([1, 96, 14, 14])\n",
    "# decoder4: torch.Size([1, 64, 28, 28])\n",
    "# decoder5: torch.Size([1, 32, 56, 56])\n",
    "# decoder6: torch.Size([1, 24, 112, 112])\n",
    "# decoder7: torch.Size([1, 16, 112, 112])\n",
    "# final deconv:  torch.Size([1, 3, 224, 224])\n",
    "# output dimension: torch.Size([1, 3, 224, 224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODER\n",
      "----Block 0----\n",
      "model size: 0.004MB\n",
      "Total Trainable Params: 928\n",
      "----Block 1----\n",
      "model size: 0.004MB\n",
      "Total Trainable Params: 896\n",
      "----Block 2----\n",
      "model size: 0.057MB\n",
      "Total Trainable Params: 13968\n",
      "----Block 3----\n",
      "model size: 0.160MB\n",
      "Total Trainable Params: 39696\n",
      "----Block 4----\n",
      "model size: 0.724MB\n",
      "Total Trainable Params: 183872\n",
      "----Block 5----\n",
      "model size: 1.182MB\n",
      "Total Trainable Params: 303168\n",
      "----Block 6----\n",
      "model size: 3.076MB\n",
      "Total Trainable Params: 795264\n",
      "----Block 7----\n",
      "model size: 1.825MB\n",
      "Total Trainable Params: 473920\n",
      "----Block 8----\n",
      "model size: 1.582MB\n",
      "Total Trainable Params: 412160\n",
      "DECODER\n",
      "----Block 0----\n",
      "model size: 1.567MB\n",
      "Total Trainable Params: 410240\n",
      "----Block 1----\n",
      "model size: 3.643MB\n",
      "Total Trainable Params: 946880\n",
      "----Block 2----\n",
      "model size: 3.474MB\n",
      "Total Trainable Params: 898432\n",
      "----Block 3----\n",
      "model size: 2.192MB\n",
      "Total Trainable Params: 564992\n",
      "----Block 4----\n",
      "model size: 0.806MB\n",
      "Total Trainable Params: 204736\n",
      "----Block 5----\n",
      "model size: 0.280MB\n",
      "Total Trainable Params: 70064\n",
      "----Block 6----\n",
      "model size: 0.130MB\n",
      "Total Trainable Params: 32192\n",
      "----Block 7----\n",
      "model size: 0.002MB\n",
      "Total Trainable Params: 454\n",
      "----Block 8----\n",
      "model size: 0.000MB\n",
      "Total Trainable Params: 87\n",
      "----UNET----\n",
      "Buffer size: 0.293 MB\n",
      "model size: 20.709MB\n",
      "Total Trainable Params: 5351949\n"
     ]
    }
   ],
   "source": [
    "print(\"ENCODER\")\n",
    "for i, block in enumerate(unet.encoder):\n",
    "    print(f\"----Block {i}----\")\n",
    "    size_all_mb = calculate_storage(block, show_buffer=False)\n",
    "    print(\"model size: {:.3f}MB\".format(size_all_mb))\n",
    "\n",
    "    total_params = count_parameters(block)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "\n",
    "\n",
    "print(\"DECODER\")\n",
    "for i, block in enumerate(unet.decoder):\n",
    "    print(f\"----Block {i}----\")\n",
    "    size_all_mb = calculate_storage(block, show_buffer=False)\n",
    "    print(\"model size: {:.3f}MB\".format(size_all_mb))\n",
    "\n",
    "    total_params = count_parameters(block)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "\n",
    "print(\"----UNET----\")\n",
    "size_all_mb = calculate_storage(unet)\n",
    "print(\"model size: {:.3f}MB\".format(size_all_mb))\n",
    "total_params = count_parameters(unet)\n",
    "print(f\"Total Trainable Params: {total_params}\")\n",
    "\n",
    "\n",
    "# Normal:\n",
    "# Buffer size: 0.145 MB\n",
    "# model size: 31.730MB\n",
    "# Total Trainable Params: 8279984\n",
    "\n",
    "# Decomposed:\n",
    "# Buffer size: 0.155 MB\n",
    "# model size: 23.437MB\n",
    "# Total Trainable Params: 6103125\n",
    "\n",
    "# Inverted Residual:\n",
    "# Buffer size: 0.293 MB\n",
    "# model size: 33.209MB\n",
    "# Total Trainable Params: 8628749"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----ENCODER----\n",
      "Buffer size: 0.131 MB\n",
      "model size: 8.614MB\n",
      "Total Trainable Params: 2223872\n",
      "----decoder----\n",
      "Buffer size: 0.162 MB\n",
      "model size: 12.095MB\n",
      "Total Trainable Params: 3128077\n"
     ]
    }
   ],
   "source": [
    "print(\"----ENCODER----\")\n",
    "size_all_mb = calculate_storage(unet.encoder)\n",
    "print(\"model size: {:.3f}MB\".format(size_all_mb))\n",
    "total_params = count_parameters(unet.encoder)\n",
    "print(f\"Total Trainable Params: {total_params}\")\n",
    "\n",
    "print(\"----decoder----\")\n",
    "size_all_mb = calculate_storage(unet.decoder)\n",
    "print(\"model size: {:.3f}MB\".format(size_all_mb))\n",
    "total_params = count_parameters(unet.decoder)\n",
    "print(f\"Total Trainable Params: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (conv0): Conv2dNormActivation(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): Dropout2d(p=0, inplace=False)\n",
       "    (3): ReLU6(inplace=True)\n",
       "  )\n",
       "  (bottleneck1): Sequential(\n",
       "    (0): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Dropout2d(p=0.0, inplace=False)\n",
       "            (3): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): Dropout2d(p=0, inplace=False)\n",
       "  )\n",
       "  (bottleneck2): Sequential(\n",
       "    (0): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): Dropout2d(p=0.0, inplace=False)\n",
       "          (3): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Dropout2d(p=0.0, inplace=False)\n",
       "            (3): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): Dropout2d(p=0, inplace=False)\n",
       "    (2): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): Dropout2d(p=0.0, inplace=False)\n",
       "          (3): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Dropout2d(p=0.0, inplace=False)\n",
       "            (3): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): Dropout2d(p=0, inplace=False)\n",
       "  )\n",
       "  (bottleneck3): Sequential(\n",
       "    (0): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): Dropout2d(p=0.0, inplace=False)\n",
       "          (3): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Dropout2d(p=0.0, inplace=False)\n",
       "            (3): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): Dropout2d(p=0, inplace=False)\n",
       "    (2): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): Dropout2d(p=0.0, inplace=False)\n",
       "          (3): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Dropout2d(p=0.0, inplace=False)\n",
       "            (3): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): Dropout2d(p=0, inplace=False)\n",
       "    (4): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): Dropout2d(p=0.0, inplace=False)\n",
       "          (3): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Dropout2d(p=0.0, inplace=False)\n",
       "            (3): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): Dropout2d(p=0, inplace=False)\n",
       "  )\n",
       "  (bottleneck4): Sequential(\n",
       "    (0): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): Dropout2d(p=0.0, inplace=False)\n",
       "          (3): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Dropout2d(p=0.0, inplace=False)\n",
       "            (3): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): Dropout2d(p=0, inplace=False)\n",
       "    (2): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): Dropout2d(p=0.0, inplace=False)\n",
       "          (3): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Dropout2d(p=0.0, inplace=False)\n",
       "            (3): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): Dropout2d(p=0, inplace=False)\n",
       "    (4): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): Dropout2d(p=0.0, inplace=False)\n",
       "          (3): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Dropout2d(p=0.0, inplace=False)\n",
       "            (3): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): Dropout2d(p=0, inplace=False)\n",
       "    (6): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): Dropout2d(p=0.0, inplace=False)\n",
       "          (3): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Dropout2d(p=0.0, inplace=False)\n",
       "            (3): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): Dropout2d(p=0, inplace=False)\n",
       "  )\n",
       "  (bottleneck5): Sequential(\n",
       "    (0): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): Dropout2d(p=0.0, inplace=False)\n",
       "          (3): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Dropout2d(p=0.0, inplace=False)\n",
       "            (3): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): Dropout2d(p=0, inplace=False)\n",
       "    (2): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): Dropout2d(p=0.0, inplace=False)\n",
       "          (3): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Dropout2d(p=0.0, inplace=False)\n",
       "            (3): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): Dropout2d(p=0, inplace=False)\n",
       "    (4): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): Dropout2d(p=0.0, inplace=False)\n",
       "          (3): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Dropout2d(p=0.0, inplace=False)\n",
       "            (3): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): Dropout2d(p=0, inplace=False)\n",
       "  )\n",
       "  (bottleneck6): Sequential(\n",
       "    (0): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): Dropout2d(p=0.0, inplace=False)\n",
       "          (3): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Dropout2d(p=0.0, inplace=False)\n",
       "            (3): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): Dropout2d(p=0.5, inplace=False)\n",
       "    (2): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): Dropout2d(p=0.0, inplace=False)\n",
       "          (3): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Dropout2d(p=0.0, inplace=False)\n",
       "            (3): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): Dropout2d(p=0.5, inplace=False)\n",
       "    (4): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): Dropout2d(p=0.0, inplace=False)\n",
       "          (3): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Dropout2d(p=0.0, inplace=False)\n",
       "            (3): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): Dropout2d(p=0.5, inplace=False)\n",
       "  )\n",
       "  (bottleneck7): Sequential(\n",
       "    (0): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): Dropout2d(p=0.0, inplace=False)\n",
       "          (3): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Dropout2d(p=0.0, inplace=False)\n",
       "            (3): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): Dropout2d(p=0.5, inplace=False)\n",
       "  )\n",
       "  (convLast): Conv2dNormActivation(\n",
       "    (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): Dropout2d(p=0.5, inplace=False)\n",
       "    (3): ReLU6(inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SNet_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
